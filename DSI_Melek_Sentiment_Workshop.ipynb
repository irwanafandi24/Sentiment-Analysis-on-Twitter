{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSI Melek Sentiment Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOGrojzybBS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2455f68b-7a0c-4f0e-9d4f-3a089eb1acee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#klik the link ==> choose your gmail account ==> click allow ==> copy the key and paste in this field"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb8ydrSJkZK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#path for the data folder\n",
        "HOME_DIR = '/content/drive/My Drive/Colab Notebooks/DSI Data/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QThyZwzolwH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import important library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJUNoNpYl7ii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cdfdc302-bcbf-48c0-96ef-518b98136d35"
      },
      "source": [
        "# Open cs file with pandas\n",
        "df = pd.read_csv(HOME_DIR+'data/ecom_op.csv', header = None)\n",
        "df = df[[0,1]]\n",
        "df.columns = ['tweet', 'label']\n",
        "df.head()\n",
        "# In NLP a tweet called as document, then the collection of document called as corpus\n",
        "# so df.iloc(0)[0] called as document"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@firstlogistics_ @bliblidotcom @bliblicare @yl...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@bliblidotcom @bliblicare tidak ada link yg pa...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@bliblidotcom min ga relavan twittnya, sekaran...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@bcaklikpay @ferdian08 @bliblidotcom jawaban p...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>waah harus hati2 nih.. kl yg ga teliti bs ke p...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0  @firstlogistics_ @bliblidotcom @bliblicare @yl...     -1\n",
              "1  @bliblidotcom @bliblicare tidak ada link yg pa...     -1\n",
              "2  @bliblidotcom min ga relavan twittnya, sekaran...     -1\n",
              "3  @bcaklikpay @ferdian08 @bliblidotcom jawaban p...     -1\n",
              "4  waah harus hati2 nih.. kl yg ga teliti bs ke p...     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOPf1tvWpFoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "655a8221-0d81-4044-ecfb-800b447ca993"
      },
      "source": [
        "#the number of document in every label\n",
        "print(df.shape)\n",
        "print(df.label.value_counts())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3144, 2)\n",
            " 0    2194\n",
            " 1     528\n",
            "-1     422\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eDJvlDKukxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "f7be44d5-4d8e-44f3-95e4-05dc0cd1777e"
      },
      "source": [
        "#look at the data before starting preprocessing\n",
        "df.tweet.head(10).values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@firstlogistics_ @bliblidotcom @bliblicare @ylki_id rasanya kecewa bgt sm jasa logistik satu ini, gak ada niat baik. ',\n",
              "       '@bliblidotcom @bliblicare tidak ada link yg pasti produknya speaker niko harga 160rbuan kirim ke manyar gresik masa kena ongkir 90rb ',\n",
              "       '@bliblidotcom min ga relavan twittnya, sekarang kan tanggal 19 april .. berarti ini twit dari masa lalu ya min? ',\n",
              "       '@bcaklikpay @ferdian08 @bliblidotcom jawaban pun klise, besok dicoba lagi bla bla bla.. klikpay emg ga pnh belajar dr pglmn kmrn2.. ',\n",
              "       'waah harus hati2 nih.. kl yg ga teliti bs ke potong harga normal.. parah jg ya @bcaklikpay dan @bliblidotcom ',\n",
              "       '@bcaklikpay @ferdian08 @bliblidotcom ane jg kena php, di chart dpt disc 65rb, proses ke klikpay kena harga normal.. ',\n",
              "       '@bcaklikpay @bliblidotcom bkn mslh coba lagi, tp krn kegoblokan sistem @bliblidotcom jd ga dpt.. ',\n",
              "       '@bliblidotcom daftar merchant kok gak ada respon ya sejak kemarin? ',\n",
              "       '@bliblicare no. pesanan: 12000300150 ku @bliblidotcom udah hampir sebulan nih, #sabar gak nongol2 jg #superkecewa ',\n",
              "       'tapi sampai saat ini belum ada yang konfirmasi ke saya.. . @bliblidotcom @bliblicare msh brthn dgn sistem lemotnya '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpbgfkAmpkL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split the data set into train and test with test_size = 0.2 (equal with 80% train and 20% testing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.tweet, df.label, test_size=0.2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmIQVpPCwgUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load stopword data from file\n",
        "stp = open(HOME_DIR+'data/twitter_stp.dic')\n",
        "stp_words = []\n",
        "items = stp.readline()\n",
        "while items != \"\":\n",
        "  stp_words.append(items[:-1]) #just read word without /n\n",
        "  items = stp.readline()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWXp0z3GxPaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5690470e-d022-4825-de96-3c3412e108be"
      },
      "source": [
        "len(stp_words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ved0J6rmEzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature extraction \n",
        "# min_df (minimal number of vocab), max_df(max number of voceb)\n",
        "# regez (token_pattern), just take character\n",
        "vect = TfidfVectorizer(min_df=0, max_df=0.9, stop_words=stp_words,\n",
        "                       token_pattern = '\\\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\\\b')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYw4u51RmJL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model for classifier, arrange the hyperparameter to get the best performance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# machine learning model\n",
        "# alpha for smoothing \n",
        "# model = MultinomialNB(alpha=0.3)\n",
        "model = RandomForestClassifier(min_samples_leaf=1, min_samples_split=3,\n",
        "                                        min_weight_fraction_leaf=0.0,\n",
        "                                        n_estimators=125)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiQ04zozmLn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create pipeline\n",
        "pipeline = Pipeline([\n",
        "                     ('vect',vect),\n",
        "                     ('clf',model)\n",
        "])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZObg5jI_qX0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "6279a4dd-3eb1-4dec-f430-b3dd944cfc50"
      },
      "source": [
        "#train the model\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['olah', 'rasa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.9, max_features=None,\n",
              "                                 min_df=0, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=['a', 'ada', 'adalah', 'ah', 'akan',\n",
              "                                             'aku', 'all', 'an', 'and', 'apa...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=3,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=125, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r274r18qk7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict the label of x_test data\n",
        "y_pred = pipeline.predict(x_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPmmM0pxqrPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "951b00e3-b908-4eff-91a7-0d36ec46f847"
      },
      "source": [
        "#evaluation the model (use confusion matrix)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.35      0.83      0.50        35\n",
            "           0       0.96      0.79      0.87       523\n",
            "           1       0.50      0.85      0.63        71\n",
            "\n",
            "    accuracy                           0.79       629\n",
            "   macro avg       0.60      0.82      0.66       629\n",
            "weighted avg       0.88      0.79      0.82       629\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe7zLSfXu1wA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "681db1f0-6220-4d7f-af94-0fd51a254e2e"
      },
      "source": [
        "#accuracy of our model\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7837837837837838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNjfcBfoq83s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4476cf8-b1a8-46cd-d568-710e9ddc10dd"
      },
      "source": [
        "#try with new data\n",
        "new_tweet = [\"rasanya kecewa bgt sm jasa logistik satu ini, gak ada niat baik\",\n",
        "             \"Proses Transaksi sangat cepat dan saya puas sekali\"]\n",
        "pipeline.predict(new_tweet)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANfiOb51mNNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dump pipeline to pickle\n",
        "pickle.dump(pipeline,open(HOME_DIR+'gridsearchvalue.pkl', 'wb'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I87vXFrYzkFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if you want to get the best parameter for your model, do hyperparameter tuning and use the best parameter to your model\n",
        "#grid search cv (your can use gridsearchcv or randomizesearchcv)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#karenda didalam clf, kasih clf untuk identifier\n",
        "params = {'vect__min_df':[0,0.05,0.1],\n",
        "          'vect__max_df':[0.9, 0.95, 0.99, 1],\n",
        "          'clf__min_samples_leaf':[1,2,3,4,5],\n",
        "          'clf__min_samples_split':[2,3],\n",
        "          'clf__min_weight_fraction_leaf':[0.0,0.05,0.1,0.15],\n",
        "          'clf__n_estimators':[100,125,150]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3je6VeP1OUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = GridSearchCV(pipeline,\n",
        "                    params,\n",
        "                    n_jobs = 5,\n",
        "                    cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5KsELvV1aio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "edc7cf5a-bf41-4faf-fda7-2ae9411f84d4"
      },
      "source": [
        "#train your model with gridsearchcv\n",
        "grid.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['olah', 'rasa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=0.99,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=0.005,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=['a', 'ada'...\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=5,\n",
              "             param_grid={'clf__min_samples_leaf': [1, 2, 3, 4, 5],\n",
              "                         'clf__min_samples_split': [2, 3],\n",
              "                         'clf__min_weight_fraction_leaf': [0.0, 0.05, 0.1,\n",
              "                                                           0.15],\n",
              "                         'clf__n_estimators': [100, 125, 150],\n",
              "                         'vect__max_df': [0.9, 0.95, 0.99, 1],\n",
              "                         'vect__min_df': [0, 0.05, 0.1]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ji8PhJr64vI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "4f65621c-196a-40c3-c0fc-54539ec0f38b"
      },
      "source": [
        "#this is the best parameter from gridsearcch\n",
        "#try to train your model above using this parameter\n",
        "grid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.9, max_features=None,\n",
              "                                 min_df=0, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=['a', 'ada', 'adalah', 'ah', 'akan',\n",
              "                                             'aku', 'all', 'an', 'and', 'apa...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=3,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=125, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz6kUo5h1fwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#precisson : akurasi orang lulus dari semua orang2 lulus\n",
        "#recall : akurasi tidak lulus dari orang2 tidak lulus"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}